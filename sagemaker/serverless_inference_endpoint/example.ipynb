{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8814b34",
   "metadata": {},
   "source": [
    "# Serverless Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1644f1",
   "metadata": {},
   "source": [
    "This example creates serverless inference endpoint using SageMaker for [GALACTICA](https://huggingface.co/facebook/galactica-125m)\n",
    "model (using mini version) for the task of generating citations for given text prompt. It shows example usage of the endpoint and its creation.\n",
    "\n",
    "## Resources\n",
    "\n",
    "| Name                        | Link |\n",
    "|-----------------------------|------|\n",
    "| Galactica                   | [https://huggingface.co/facebook/galactica-125m](https://huggingface.co/facebook/galactica-125m) |\n",
    "| Galactica                   | [https://arxiv.org/abs/2211.09085](https://arxiv.org/abs/2211.09085) |\n",
    "| AWS SageMaker Serverless    | [https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html) |\n",
    "| AWS SageMaker SDK           | [https://sagemaker.readthedocs.io/en/stable/](https://sagemaker.readthedocs.io/en/stable/) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4386d9",
   "metadata": {},
   "source": [
    "### Permissions\n",
    "\n",
    "This code is used to create roles and SageMaker session, it can be dependant on the AWS account, role and region used.\n",
    "For more details refer to the [AWS documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) and [SageMaker Serverless Examples](https://github.com/aws/amazon-sagemaker-examples/blob/main/serverless-inference/Serverless-Inference-Walkthrough.ipynb).\n",
    "\n",
    "*Note*: Run pip install only when necessary, depending on the environment You are using (for SageMaker notebooks this is not required.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf621569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install boto3 --upgrade\n",
    "# !pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c22e8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\n",
    "session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "038103e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_BUCKET = \"\"  # TODO: add S3 bucket where model is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1173887d",
   "metadata": {},
   "source": [
    "Setup required imports and set logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5bdae35-bb92-4c66-a796-81c28ff9e5b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from endpoint import ServerlessEndpoint\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9ebcc-ed5c-4559-84d7-3469b389cd72",
   "metadata": {
    "tags": []
   },
   "source": [
    "HuggingFace environment variables, request and response size are increased with respect to defaults, since the model takes text as input and produces text, which might have varying length and threfore varying size of the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf6177b-c412-4a5f-92c1-4cd8b0b4ae84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_ENV = {\n",
    "    \"TS_MAX_RESPONSE_SIZE\": \"13107000\",\n",
    "    \"TS_MAX_REQUEST_SIZE\": \"13107000\",\n",
    "    \"MMS_MAX_RESPONSE_SIZE\": \"13107000\",\n",
    "    \"MMS_MAX_REQUEST_SIZE\": \"13107000\",\n",
    "    \"MMS_WORKERS_PER_MODEL\": \"4\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b2601",
   "metadata": {},
   "source": [
    "Creates the endpoint for model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b70daf-3e03-4449-9c2e-9643c1dc7f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint = ServerlessEndpoint(\n",
    "    model_name=\"galactica-125m\",\n",
    "    model_dir=f\"s3://{MODEL_BUCKET}/galactica-mini/galactica-citation-prediction.tar.gz\",\n",
    "    role_arn=role,\n",
    "    env=HF_ENV,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b6d30",
   "metadata": {},
   "source": [
    "Setup the endpoint. This takes a while, in this step model is added to SageMaker models section (as Docker image) and the endpoint is deployed (on AWS Lambda underneath).\n",
    "\n",
    "Dash logging is produced by AWS SDK, which is not too informative (should be 4 dashes and exclamation point `----!` for ready endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36ecddc-d711-487a-883e-5bc000a4fc5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to CPU type when using serverless inference\n",
      "INFO:sagemaker:Creating model with name: galactica-125m\n",
      "INFO:sagemaker:Creating endpoint-config with name galactica-125m-2023-08-04-07-56-15-555\n",
      "INFO:sagemaker:Creating endpoint with name galactica-125m-2023-08-04-07-56-15-555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:endpoint:Created endpoint!\n"
     ]
    }
   ],
   "source": [
    "endpoint.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75768a",
   "metadata": {},
   "source": [
    "Run a few examples, the Galactica model should be able to predict the publication name for given prompt relatively well. For niche topics it will not work well, but for general topics it should be able to predict the publication name with high accuracy. For Transformer it correctly identifies *\"Attention is All You Need\"* with the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30684de8-54ff-457d-9b0a-c233ecd7b13b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 ms, sys: 0 ns, total: 15.6 ms\n",
      "Wall time: 4.93 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Attention is All you Need, Vaswani.# 3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time endpoint(\"The Transformer architecture\")  # correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f64cba-a134-448f-8161-1e6ef22f13c9",
   "metadata": {},
   "source": [
    "Second run after the endpoint is warm is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda43a86-5daf-44f1-83e3-0ff0c01321e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Adam: A Method for Stochastic Optimization, Kingma, and a batch size of'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time endpoint(\"Adam optimizer\")  # correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "525a7d1a-0c4f-40ac-a681-07768de9f828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 950 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation, Cho,'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time endpoint(\"LSTM\")  # not the original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a91ea2bf-a065-46fd-b735-a9f1a1c826e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 859 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A Fast and Accurate Method to Estimate Folding Energy, Kabsch'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time endpoint(\"AlphaFold\")  # wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c42abca",
   "metadata": {},
   "source": [
    "Remove the endpoint and its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea9d4c1e-77b2-44fa-9655-4a6bc6792178",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: galactica-125m\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: galactica-125m-2023-08-04-07-56-15-555\n",
      "INFO:sagemaker:Deleting endpoint with name: galactica-125m-2023-08-04-07-56-15-555\n",
      "INFO:endpoint:Cleaned up endpoint!\n"
     ]
    }
   ],
   "source": [
    "endpoint.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb3e43-76ed-493c-84dc-74d48ab58e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
