{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "193bc860-5d55-44e6-adf7-819cac273edd",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "This notebook, contains the flow of data processing application. The goal of the app is to use Glue ETL data Glue Data Catalog to register data from Stack Overflow survey into PostgreSQL database. All required resources can be created using the `infra` defined in this repository. There is no external orchestration of the flow, but this notebook can be treated as guideline how to trigger required steps via AWS Console or Python.\n",
    "\n",
    "*Note*: Resources are linked in markdown cells, where needed. Detailed description of the infrastructure set-up is given in README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f72775-8365-4a81-ae93-323aa2227afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "from db.client import DBClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c16f2a3-14e3-4b2c-9a2e-88c1a6d95ce4",
   "metadata": {},
   "source": [
    "# Raw Data\n",
    "\n",
    "First step will be to simply upload CSV file to S3. AWS CLI can be used for this, to avoid loading file. For all AWS communication profile `pwr` will be used. For reference how to use credentials file for AWS go to https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9009af-e505-4bea-902b-7c39c2c37b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --profile pwr ../../data/stack-overflow-developer-survey-2023/survey_results_public_2023.csv s3://dps-glue-data/raw/2023/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dedf6b-2273-4842-98b8-fc6ec7d8cd33",
   "metadata": {},
   "source": [
    "The learning lab uses `us-east-1` region. To create `boto3.client` with given region and profile settings, `boto3.Session` needs to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a953ec-8945-4b52-bc07-4fe65b38c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(profile_name=\"pwr\", region_name=\"us-east-1\")\n",
    "glue = session.client(\"glue\")\n",
    "athena = session.client(\"athena\")\n",
    "ssm = session.client(\"ssm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80607342-3931-4ddb-b983-c45319d52cb5",
   "metadata": {},
   "source": [
    "After uploading raw data, we want it to be registered in glue database. To do this, crawler will be run from boto3. <br> The only parameter is crawler name, its input and output are configured in the terraform during the resource creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc2980-be95-4d16-8ca9-dbe77866fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue.start_crawler(Name=\"raw-data-crawler\")  # Exceptions will propagate to notebook, if they happen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d431e11-2f7a-4572-994b-6bb2e26a0b01",
   "metadata": {},
   "source": [
    "To verify the run, checkout AWS Console with following path `AWS Glue` -> `Data Catalog` -> `Crawlers` -> `raw-data-crawler` -> `Crawler runs`.\n",
    "\n",
    "Additionally, we can checkout, how the table structure looks in Athena. The `QueryString` is the SQL query, which is executed on serverless database (Athena). The engine reads data directly from S3, applies schema (registered by the glue crawler) and executes the query. Results are stored as JSON on S3 or can be accessed in the AWS Console with SQL editor.\n",
    "\n",
    "https://docs.aws.amazon.com/athena/latest/ug/getting-started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224447c4-6288-4e69-af87-9dca15bad522",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = athena.start_query_execution(\n",
    "    # survery_db.raw is the table created by Glue\n",
    "    QueryString=\"SELECT * FROM raw LIMIT 10\",\n",
    "    QueryExecutionContext={\"Database\": \"survey_db\"},\n",
    "    ResultConfiguration={\"OutputLocation\": \"s3://dps-glue-data/athena/\"}\n",
    ")\n",
    "query_execution_id = response[\"QueryExecutionId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c21aa-433c-4000-a191-ca7f8e101f59",
   "metadata": {},
   "source": [
    "In python script, this could be triggered in `while` loop, until the execution is finished. In the notebook, in case the query is still `\"RUNNING\"`, rerun the cell until `\"SUCCEEDED\"` status or simply wait. The query should run quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd5809-ddaf-4c5d-ba1a-591368c76b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = athena.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "status[\"QueryExecution\"][\"Status\"][\"State\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e42a6-7fb8-4953-8ca7-9ba3dc177e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boto3_session needs to be passed, since non-default profile is used\n",
    "raw_data = wr.athena.get_query_results(query_execution_id=query_execution_id, boto3_session=session)\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c2dc9-749b-494e-90f1-b9fa7d2c81e1",
   "metadata": {},
   "source": [
    "# Transform Data\n",
    "\n",
    "After registering data in Glue, Athena can be used to analyse the CSV file. Additionally, Glue supports ETL jobs using PySpark, which can read and write the Glue Data Catalog. Two such jobs are defined in `src/glue`, one for ingestion and one for data normalization to insert it into RDS. The first job can be triggered from python, passing required arguments.\n",
    "\n",
    "*Note*: Arguments are defined in terraform with default values, those can be over-written by using boto3 client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171721c-ad0d-4745-ba73-cf29ddc72970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only default arguments are used, so just the job name is passed.\n",
    "response = glue.start_job_run(JobName=\"dps-ingest\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f3e49-f38b-4361-aaf9-1be759a2eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = glue.get_job_run(JobName=\"dps-ingest\", RunId=response[\"JobRunId\"])\n",
    "status[\"JobRun\"][\"JobRunState\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130e3b2-4fbf-47da-8d51-08625debb4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = athena.start_query_execution(\n",
    "    # survery_db.raw is the table created by Glue\n",
    "    QueryString=\"SELECT * FROM processed LIMIT 10\",\n",
    "    QueryExecutionContext={\"Database\": \"survey_db\"},\n",
    "    ResultConfiguration={\"OutputLocation\": \"s3://dps-glue-data/athena/\"}\n",
    ")\n",
    "query_execution_id = response[\"QueryExecutionId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d11f3-063d-473f-81ad-aa73b81a7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = athena.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "status[\"QueryExecution\"][\"Status\"][\"State\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56335ec-35aa-4913-b579-39e2317bf4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boto3_session needs to be passed, since non-default profile is used\n",
    "processed_data = wr.athena.get_query_results(query_execution_id=query_execution_id, boto3_session=session)\n",
    "processed_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f531ef-9952-4508-88b5-006b53781150",
   "metadata": {},
   "source": [
    "# Normalize\n",
    "\n",
    "The second ETL transform will convert the `processed` schema into normalize SQL tables, 3 tables and bridge tables.\n",
    "\n",
    "*Note*: Using Spark to convert such data into SQL schema is not typically used, this is meant more for deminstration of RDS. The SQL schema given in the RDS is not following typical analytical best practices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8197b-d55c-437b-8bdb-ad57a4596d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only default arguments are used, so just the job name is passed.\n",
    "response = glue.start_job_run(JobName=\"dps-transform\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9721c6-aa0c-48b1-ba09-55fe7a862d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = glue.get_job_run(JobName=\"dps-transform\", RunId=response[\"JobRunId\"])\n",
    "status[\"JobRun\"][\"JobRunState\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c8af73-3939-4937-890d-1436ed8c389d",
   "metadata": {},
   "source": [
    "# Setup SQL\n",
    "\n",
    "To connect to RDS only IP given by `vpn_ip` in the terraform config. This is passed via environment variable and it is a secret variable along with the database password. This is a safety features, implementing IP whitelisting using the EC2 Security Group. More details on: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html.\n",
    "\n",
    "*Note*: IP given in the config is under VPN for extra safety. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b4703-10cd-4996-b1c4-c94e4248b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = DBClient.from_ssm(prefix=\"db\", client=ssm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7bda3a-bfac-4d90-816b-e738634c3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client.list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c35731-381b-40a1-8685-cad1bf5945bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client.select(table=\"answers\", limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749f2655-bc04-4223-9893-882e028106cd",
   "metadata": {},
   "source": [
    "Initially the fresh database is empty. Schema can be created by executing DDL SQL script from the file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60afba6c-aa93-4eb1-bfe0-939fa1eb7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"db/sql/schema.sql\") as f:\n",
    "    query = f.read()\n",
    "\n",
    "db_client.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac5750a-ed05-47e3-95ab-51d250dfc069",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client.list_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad2f6f-3891-44e3-98a8-76324814d1be",
   "metadata": {},
   "source": [
    "After adding the schema, we can use PySpark script adding the S3 data into SQL. Data will be briefly kept in the memory of the computer this is running on, which means it will be transferred out and back into AWS.\n",
    "\n",
    "*Note*: PySpark required additonal drivers to run the SQL operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a031b200-1925-4c34-9c4b-fa5f62279c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python db/insert.py --profile pwr --region us-east-1 --s3-dir dps-glue-data/normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6328d-3daf-49c2-be44-e3ac8c788f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client.select(table=\"countries\", limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c13f1a-bf78-4a70-8bee-d1b53adb8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client.select(table=\"answers\", limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152d914-12ca-45f3-b5e8-7d12a5bac5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
